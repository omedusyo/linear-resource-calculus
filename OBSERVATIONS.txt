* Linear pattern matching seems to very different when compared to Cartesian case.
* In dynamically tagged version of the linear calculus (without explicit types),
  the objects have to have an explicit list of resources that they capture from the current environment.
  In static version this can probably be infered at static time.
  But this "moved" list of resources does have a feel of instance variables from classical OOP.
* In cartesian calculus, when using a variable, you're just doing a non-destructive lookup in the current environment.
  In linear calculus, variable usage is a true use, i.e. it tries to find the bound value in current environment,
  and when it finds it, it literally rips the binding out of the environment. Hence the environment is changed.
  If it fails to find the bindings, this is considered an error, an analogue of dereferencing a null pointer,
  which with static type system is impossible.
  I wonder if Hoare logic is really about this kinda linear calculus.
  Linear calculus seems to be doing "mutation" right. In fact it seems to show that mutation is not the correct fundamental concept.
  The correct fundamental concept is that of "becoming" - A thing becomes some other thing, and it may happen that the type changes too.
  And mutation is just a special case, where the types happen to stay the same.
* Sending "messages" to objects is very different when it comes to dynamically tagged vs statically typed calculi.
  In dynamically typed version these objects don't seem to bring anything new, they could be simulated by a closure that
  takes in the message as input, then simulates `receive` by pattern matching on the input.
  When we have static types, the messages of objects are not like usual data. Which message is sent when should be derivable at static time!
  This is like a limited version of dependent function types, where the domain of the family (bundle/fibration) is just a very finite,
  statically known set. With this, we actually gain more expressive power than just with pattern matching - where without dependent types,
  each branch has to have the same type.
* In the combined calculus, cartesian values can be thought of as linear values too.
  Interestingly it is trivial to discard/duplicate them. Those are very cheap operations.
* In the Linear calculus, we have two basic ways of "using" a variable.
  Actual use/move, which is `%x`, and attempted clone usage, which is `clone x`.
  First one rips the binding of variable to value out of the environment, and moves it to the user.
  We could read % as `move`. That's pretty neat. Maybe I should introduce that as a syntactic sugar `move x` or `take x`
* Having multiple ways of using a variable... move, clone, drop,... note how e.g. clone introduces ordering issues.
    // assumming left to right evaluation order
    let { x = 123 . [clone x, %x] } // is fine
    let { x = 123 . [%x, clone x] } // is not fine, it will crash
  Now order of evaluation is important.

* sending 'static' messages to 'dynamic' coinductive objects  vs  matching 'dynamic' inductive data with 'static' branches.
    match-ind[%dynamic-ind-data, static-branch]     vs     match-coind[static-message, %dynamic-coind-obj]
  Syntactically we have different names (and for mathc-coind, we flip the arguments because otherwise we would be plagued with syntactic ambiguity in the parser)
    match[%dynamic-ind-data, static-branch]     vs     send[%dynamic-coind-obj, static-message]
  The ordering is unfortunate. Would be cool to have syntax that can capture the symmetry.

  Also in inductive case, all of the inductive data must be consumed at once, since the branches are static.
  Because of the 'static' branches, we have to go as deeply as possible in the match so that we are not left with a 'static' branch at compile time (which suddenly sounds like it could be resolved via dynamic object...)

  In the coinductive case, we can send the messages 'partially'... and still get reasonable intermediate objects.
  Consider, instead of sending
    send %obj @msg0 @msg1 [12, 3, 4] @msg2
  we can do
    let obj = send %obj @msg0 @msg1
    let obj = send %obj [12, 3, 4] @msg2

  How could %obj look like? Something like
    obj {
    | @msg0 { @msg1 [x, y, z] @msg2 .
            | ...
            }
        f[x, y, z]
    | ...
    }
  And with the partiality of send in mind that's equivalent to
    obj {
    | @msg0 . obj { 
              | @msg1 . obj { [x, y, z] . obj { @msg2 . ... } } 
              | ...
              }
    | ...
    }

* Get your abstract syntax right - and it should be very simple. Then you can build concrete syntax on top of it.
* For a type theory, if you don't understand how cut/let works, you don't understand your type theory. That's the most fundamental thing.


* In Cartesian World you have shallow cloning.
  In Linear World, you don't have deep cloning universally... but that's not the correct analogue of shallow cloning...
  Cartesian                            vs        Linear
    $x shallow-clone                                %a  move
    shallow-clone : Env -> (Env, Env)               split : Env -> (Env, Env)     // this is only informal analogy, formally this is more complex
    let { x = e . body }                            cut { a = e . body }
  I bet you can do a whole theory parametrized by what sort of environment you want, which is probably some sort of monoid/comonoid...
  To get Cartesian programming, you have the cartesian environments which are made to be trivially clonable.
  To get Linear programming, you have the linear environments, which seem to be captured by the monoid of multisubsets of values.
  To get "Ordered" programming (not sure what name would be appropriate for this yet), you take environemnts made from List[Value].
